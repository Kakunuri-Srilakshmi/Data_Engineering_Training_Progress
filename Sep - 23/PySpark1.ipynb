{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MX3YCYnR0IpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad71a2d0-33d7-4a5f-f338-8beadefcfdf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Product-Order-Example\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vFyqDFHROlXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating DataFrames**"
      ],
      "metadata": {
        "id": "YeaH0Qq90387"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Product data\n",
        "product_data = [\n",
        "    (101, \"Laptop\", \"Electronics\", 55000),\n",
        "    (102, \"Mobile Phone\", \"Electronics\", 25000),\n",
        "    (103, \"Chair\", \"Furniture\", 5000),\n",
        "    (104, \"Book\", \"Stationery\", 300),\n",
        "    (105, \"Headphones\", \"Electronics\", 3000)\n",
        "]\n",
        "\n",
        "product_cols = [\"product_id\", \"name\", \"category\", \"price\"]\n",
        "product_df = spark.createDataFrame(product_data, product_cols)\n",
        "\n",
        "# Order data\n",
        "order_data = [\n",
        "    (201, 101, 2, \"Rahul Sharma\"),\n",
        "    (202, 102, 1, \"Priya Singh\"),\n",
        "    (203, 103, 4, \"Aman Kumar\"),\n",
        "    (204, 104, 10, \"Sneha Reddy\"),\n",
        "    (205, 101, 1, \"Arjun Mehta\"),\n",
        "    (206, 105, 3, \"Rahul Sharma\"),\n",
        "    (207, 106, 1, \"Ghost Customer\")  # Order with product not in catalog\n",
        "]\n",
        "\n",
        "order_cols = [\"order_id\", \"product_id\", \"quantity\", \"customer\"]\n",
        "order_df = spark.createDataFrame(order_data, order_cols)\n",
        "\n",
        "# Show both\n",
        "product_df.show()\n",
        "order_df.show()"
      ],
      "metadata": {
        "id": "jLTIes9DArq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d175a08a-6919-4fd1-d95c-dfd67b9d5351"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+-----+\n",
            "|product_id|        name|   category|price|\n",
            "+----------+------------+-----------+-----+\n",
            "|       101|      Laptop|Electronics|55000|\n",
            "|       102|Mobile Phone|Electronics|25000|\n",
            "|       103|       Chair|  Furniture| 5000|\n",
            "|       104|        Book| Stationery|  300|\n",
            "|       105|  Headphones|Electronics| 3000|\n",
            "+----------+------------+-----------+-----+\n",
            "\n",
            "+--------+----------+--------+--------------+\n",
            "|order_id|product_id|quantity|      customer|\n",
            "+--------+----------+--------+--------------+\n",
            "|     201|       101|       2|  Rahul Sharma|\n",
            "|     202|       102|       1|   Priya Singh|\n",
            "|     203|       103|       4|    Aman Kumar|\n",
            "|     204|       104|      10|   Sneha Reddy|\n",
            "|     205|       101|       1|   Arjun Mehta|\n",
            "|     206|       105|       3|  Rahul Sharma|\n",
            "|     207|       106|       1|Ghost Customer|\n",
            "+--------+----------+--------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformations**"
      ],
      "metadata": {
        "id": "lk9c8GhT11Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select specific columns\n",
        "product_df.select(\"name\", \"price\").show()\n",
        "\n",
        "# filter products with price >10,000\n",
        "product_df.filter(product_df[\"price\"] > 10000).show()\n",
        "\n",
        "# order products by price descending\n",
        "product_df.orderBy(product_df[\"price\"].desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkHxzr6StEW7",
        "outputId": "53c0e568-d5e4-4573-b5c1-ff330aed57b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|        name|price|\n",
            "+------------+-----+\n",
            "|      Laptop|55000|\n",
            "|Mobile Phone|25000|\n",
            "|       Chair| 5000|\n",
            "|        Book|  300|\n",
            "|  Headphones| 3000|\n",
            "+------------+-----+\n",
            "\n",
            "+----------+------------+-----------+-----+\n",
            "|product_id|        name|   category|price|\n",
            "+----------+------------+-----------+-----+\n",
            "|       101|      Laptop|Electronics|55000|\n",
            "|       102|Mobile Phone|Electronics|25000|\n",
            "+----------+------------+-----------+-----+\n",
            "\n",
            "+----------+------------+-----------+-----+\n",
            "|product_id|        name|   category|price|\n",
            "+----------+------------+-----------+-----+\n",
            "|       101|      Laptop|Electronics|55000|\n",
            "|       102|Mobile Phone|Electronics|25000|\n",
            "|       103|       Chair|  Furniture| 5000|\n",
            "|       105|  Headphones|Electronics| 3000|\n",
            "|       104|        Book| Stationery|  300|\n",
            "+----------+------------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aggregations**"
      ],
      "metadata": {
        "id": "IzoMO7_a2FTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total quantity ordered per product\n",
        "order_df.groupBy(\"product_id\"). sum(\"quantity\").show()\n",
        "\n",
        "# Count of orders per customer\n",
        "order_df.groupBy(\"customer\").count().show()\n",
        "\n",
        "# Average price per category\n",
        "product_df.groupBy(\"category\").avg(\"price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ4mf2rEuogg",
        "outputId": "8907e791-3776-4bff-9d34-2ebe015acb4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|sum(quantity)|\n",
            "+----------+-------------+\n",
            "|       103|            4|\n",
            "|       101|            3|\n",
            "|       102|            1|\n",
            "|       104|           10|\n",
            "|       106|            1|\n",
            "|       105|            3|\n",
            "+----------+-------------+\n",
            "\n",
            "+--------------+-----+\n",
            "|      customer|count|\n",
            "+--------------+-----+\n",
            "|    Aman Kumar|    1|\n",
            "|  Rahul Sharma|    2|\n",
            "|   Priya Singh|    1|\n",
            "|   Arjun Mehta|    1|\n",
            "|Ghost Customer|    1|\n",
            "|   Sneha Reddy|    1|\n",
            "+--------------+-----+\n",
            "\n",
            "+-----------+------------------+\n",
            "|   category|        avg(price)|\n",
            "+-----------+------------------+\n",
            "|Electronics|27666.666666666668|\n",
            "| Stationery|             300.0|\n",
            "|  Furniture|            5000.0|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Joins**"
      ],
      "metadata": {
        "id": "fG1PVeuC4CUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Join : Orders with product details\n",
        "order_df.join(product_df, order_df[\"product_id\"] == product_df[\"product_id\"], \"inner\").show()\n",
        "\n",
        "# Left Join: All orders, even if product not found\n",
        "order_df.join(product_df, order_df[\"product_id\"] == product_df[\"product_id\"], \"left\").show()\n",
        "\n",
        "# Right Join: All products, even if never ordered\n",
        "order_df.join(product_df, order_df[\"product_id\"] == product_df[\"product_id\"], \"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtT8m7V-0flw",
        "outputId": "874b1c6f-f81c-469f-cb57-2a5be6147893"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|order_id|product_id|quantity|    customer|product_id|        name|   category|price|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|     201|       101|       2|Rahul Sharma|       101|      Laptop|Electronics|55000|\n",
            "|     205|       101|       1| Arjun Mehta|       101|      Laptop|Electronics|55000|\n",
            "|     202|       102|       1| Priya Singh|       102|Mobile Phone|Electronics|25000|\n",
            "|     203|       103|       4|  Aman Kumar|       103|       Chair|  Furniture| 5000|\n",
            "|     204|       104|      10| Sneha Reddy|       104|        Book| Stationery|  300|\n",
            "|     206|       105|       3|Rahul Sharma|       105|  Headphones|Electronics| 3000|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "\n",
            "+--------+----------+--------+--------------+----------+------------+-----------+-----+\n",
            "|order_id|product_id|quantity|      customer|product_id|        name|   category|price|\n",
            "+--------+----------+--------+--------------+----------+------------+-----------+-----+\n",
            "|     203|       103|       4|    Aman Kumar|       103|       Chair|  Furniture| 5000|\n",
            "|     201|       101|       2|  Rahul Sharma|       101|      Laptop|Electronics|55000|\n",
            "|     202|       102|       1|   Priya Singh|       102|Mobile Phone|Electronics|25000|\n",
            "|     204|       104|      10|   Sneha Reddy|       104|        Book| Stationery|  300|\n",
            "|     207|       106|       1|Ghost Customer|      NULL|        NULL|       NULL| NULL|\n",
            "|     206|       105|       3|  Rahul Sharma|       105|  Headphones|Electronics| 3000|\n",
            "|     205|       101|       1|   Arjun Mehta|       101|      Laptop|Electronics|55000|\n",
            "+--------+----------+--------+--------------+----------+------------+-----------+-----+\n",
            "\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|order_id|product_id|quantity|    customer|product_id|        name|   category|price|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|     205|       101|       1| Arjun Mehta|       101|      Laptop|Electronics|55000|\n",
            "|     201|       101|       2|Rahul Sharma|       101|      Laptop|Electronics|55000|\n",
            "|     202|       102|       1| Priya Singh|       102|Mobile Phone|Electronics|25000|\n",
            "|     203|       103|       4|  Aman Kumar|       103|       Chair|  Furniture| 5000|\n",
            "|     204|       104|      10| Sneha Reddy|       104|        Book| Stationery|  300|\n",
            "|     206|       105|       3|Rahul Sharma|       105|  Headphones|Electronics| 3000|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SQL Queries**"
      ],
      "metadata": {
        "id": "2ZtwFzfm98oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register as temp views\n",
        "product_df.createOrReplaceTempView(\"products\")\n",
        "order_df.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "# Query: Total revenue per product\n",
        "spark.sql(\"\"\"\n",
        "SELECT o.product_id, p.name, SUM(o.quantity * p.price) AS total_revenue\n",
        "FROM orders o\n",
        "JOIN products p ON o.product_id = p.product_id\n",
        "GROUP BY o.product_id, p.name\n",
        "\"\"\").show()\n",
        "\n",
        "# Query: Top 2 customers by total quantity\n",
        "spark.sql(\"\"\"\n",
        "SELECT customer, SUM(quantity) AS total_quantity\n",
        "FROM orders\n",
        "GROUP BY customer\n",
        "ORDER BY total_quantity DESC\n",
        "LIMIT 2\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Ofcc753i1x",
        "outputId": "1691557b-8fde-44ef-98a9-6b0cd55c1e7b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|        name|total_revenue|\n",
            "+----------+------------+-------------+\n",
            "|       101|      Laptop|       165000|\n",
            "|       102|Mobile Phone|        25000|\n",
            "|       103|       Chair|        20000|\n",
            "|       104|        Book|         3000|\n",
            "|       105|  Headphones|         9000|\n",
            "+----------+------------+-------------+\n",
            "\n",
            "+------------+--------------+\n",
            "|    customer|total_quantity|\n",
            "+------------+--------------+\n",
            "| Sneha Reddy|            10|\n",
            "|Rahul Sharma|             5|\n",
            "+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task - 1**"
      ],
      "metadata": {
        "id": "IzF4DNZ8J2E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Student data\n",
        "students_data = [\n",
        "    (1, \"Rahul Sharma\", 20, \"Bangalore\"),\n",
        "    (2, \"Priya Singh\", 21, \"Delhi\"),\n",
        "    (3, \"Aman Kumar\", 19, \"Hyderabad\"),\n",
        "    (4, \"Sneha Reddy\", 22, \"Chennai\"),\n",
        "    (5, \"Arjun Mehta\", 23, \"Mumbai\"),\n",
        "    (6, \"Divya Nair\", 20, None)  # Student without city\n",
        "]\n",
        "students_cols = [\"student_id\", \"name\", \"age\", \"city\"]\n",
        "students_df = spark.createDataFrame(students_data, students_cols)\n",
        "\n",
        "# Course data\n",
        "courses_data = [\n",
        "    (101, \"Python\", \"Programming\"),\n",
        "    (102, \"Data Science\", \"Analytics\"),\n",
        "    (103, \"Databases\", \"Technology\"),\n",
        "    (104, \"Business Studies\", \"Management\")\n",
        "]\n",
        "courses_cols = [\"course_id\", \"course_name\", \"category\"]\n",
        "courses_df = spark.createDataFrame(courses_data, courses_cols)\n",
        "\n",
        "# Enrollment data\n",
        "enrollment_data = [\n",
        "    (1, 101, \"A\"),\n",
        "    (2, 101, \"B\"),\n",
        "    (3, 102, \"A\"),\n",
        "    (4, 103, \"C\"),\n",
        "    (5, 102, \"B\"),\n",
        "    (7, 104, \"A\")  # Enrollment with non-existent student\n",
        "]\n",
        "enrollment_cols = [\"student_id\", \"course_id\", \"grade\"]\n",
        "enrollment_df = spark.createDataFrame(enrollment_data, enrollment_cols)\n",
        "\n",
        "# Show all DataFrames\n",
        "students_df.show()\n",
        "courses_df.show()\n",
        "enrollment_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG4BSf-D9IWF",
        "outputId": "ae8adcfb-19de-4a7f-b298-de2574988e5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+---+---------+\n",
            "|student_id|        name|age|     city|\n",
            "+----------+------------+---+---------+\n",
            "|         1|Rahul Sharma| 20|Bangalore|\n",
            "|         2| Priya Singh| 21|    Delhi|\n",
            "|         3|  Aman Kumar| 19|Hyderabad|\n",
            "|         4| Sneha Reddy| 22|  Chennai|\n",
            "|         5| Arjun Mehta| 23|   Mumbai|\n",
            "|         6|  Divya Nair| 20|     NULL|\n",
            "+----------+------------+---+---------+\n",
            "\n",
            "+---------+----------------+-----------+\n",
            "|course_id|     course_name|   category|\n",
            "+---------+----------------+-----------+\n",
            "|      101|          Python|Programming|\n",
            "|      102|    Data Science|  Analytics|\n",
            "|      103|       Databases| Technology|\n",
            "|      104|Business Studies| Management|\n",
            "+---------+----------------+-----------+\n",
            "\n",
            "+----------+---------+-----+\n",
            "|student_id|course_id|grade|\n",
            "+----------+---------+-----+\n",
            "|         1|      101|    A|\n",
            "|         2|      101|    B|\n",
            "|         3|      102|    A|\n",
            "|         4|      103|    C|\n",
            "|         5|      102|    B|\n",
            "|         7|      104|    A|\n",
            "+----------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformations/Task - 1**"
      ],
      "metadata": {
        "id": "bTrKdY4wKUf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Select all student names and their cities\n",
        "students_df.select(\"name\", \"city\").show()\n",
        "\n",
        "# 2. Find students who are older than 20\n",
        "students_df.filter(students_df[\"age\"] > 20).show()\n",
        "\n",
        "# 3. List all courses under the \"Analytics\" category\n",
        "courses_df.filter(courses_df[\"category\"] == \"Analytics\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2gAql7pJv1G",
        "outputId": "da9f5377-eec1-4e6f-b89f-07561eb413ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+\n",
            "|        name|     city|\n",
            "+------------+---------+\n",
            "|Rahul Sharma|Bangalore|\n",
            "| Priya Singh|    Delhi|\n",
            "|  Aman Kumar|Hyderabad|\n",
            "| Sneha Reddy|  Chennai|\n",
            "| Arjun Mehta|   Mumbai|\n",
            "|  Divya Nair|     NULL|\n",
            "+------------+---------+\n",
            "\n",
            "+----------+-----------+---+-------+\n",
            "|student_id|       name|age|   city|\n",
            "+----------+-----------+---+-------+\n",
            "|         2|Priya Singh| 21|  Delhi|\n",
            "|         4|Sneha Reddy| 22|Chennai|\n",
            "|         5|Arjun Mehta| 23| Mumbai|\n",
            "+----------+-----------+---+-------+\n",
            "\n",
            "+---------+------------+---------+\n",
            "|course_id| course_name| category|\n",
            "+---------+------------+---------+\n",
            "|      102|Data Science|Analytics|\n",
            "+---------+------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aggregations/Task - 1**"
      ],
      "metadata": {
        "id": "HcGIshfJKg4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, max, min, count\n",
        "\n",
        "# 1. Count how many students are enrolled in each course\n",
        "enrollment_df.groupBy(\"course_id\").count().show()\n",
        "\n",
        "# 2. Find the average age of students per city\n",
        "students_df.groupBy(\"city\").agg(avg(\"age\").alias(\"avg_age\")).show()\n",
        "\n",
        "# 3. Get the maximum and minimum age of students\n",
        "students_df.agg(max(\"age\").alias(\"max_age\"), min(\"age\").alias(\"min_age\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exaqibi1KCCi",
        "outputId": "1ad9ae60-837e-4e35-f8df-ef6e25039ec2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|course_id|count|\n",
            "+---------+-----+\n",
            "|      101|    2|\n",
            "|      102|    2|\n",
            "|      103|    1|\n",
            "|      104|    1|\n",
            "+---------+-----+\n",
            "\n",
            "+---------+-------+\n",
            "|     city|avg_age|\n",
            "+---------+-------+\n",
            "|Bangalore|   20.0|\n",
            "|    Delhi|   21.0|\n",
            "|Hyderabad|   19.0|\n",
            "|  Chennai|   22.0|\n",
            "|     NULL|   20.0|\n",
            "|   Mumbai|   23.0|\n",
            "+---------+-------+\n",
            "\n",
            "+-------+-------+\n",
            "|max_age|min_age|\n",
            "+-------+-------+\n",
            "|     23|     19|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Joins / Task - 1**"
      ],
      "metadata": {
        "id": "qLZ6wq3_Kn3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Join students with enrollments to see which student took which course\n",
        "students_df.join(enrollment_df, \"student_id\", \"inner\").show()\n",
        "\n",
        "# 2. Left join enrollments with courses to get course details\n",
        "enrollment_df.join(courses_df, \"course_id\", \"left\").show()\n",
        "\n",
        "# 3. Find students who are not enrolled in any course\n",
        "students_df.join(enrollment_df, \"student_id\", \"left_anti\").show()\n",
        "\n",
        "# 4. Find courses with no students enrolled\n",
        "courses_df.join(enrollment_df, \"course_id\", \"left_anti\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpaHTWnlKFZN",
        "outputId": "56f7e453-f873-43ff-a95b-5d0309c286eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+---+---------+---------+-----+\n",
            "|student_id|        name|age|     city|course_id|grade|\n",
            "+----------+------------+---+---------+---------+-----+\n",
            "|         1|Rahul Sharma| 20|Bangalore|      101|    A|\n",
            "|         2| Priya Singh| 21|    Delhi|      101|    B|\n",
            "|         3|  Aman Kumar| 19|Hyderabad|      102|    A|\n",
            "|         4| Sneha Reddy| 22|  Chennai|      103|    C|\n",
            "|         5| Arjun Mehta| 23|   Mumbai|      102|    B|\n",
            "+----------+------------+---+---------+---------+-----+\n",
            "\n",
            "+---------+----------+-----+----------------+-----------+\n",
            "|course_id|student_id|grade|     course_name|   category|\n",
            "+---------+----------+-----+----------------+-----------+\n",
            "|      101|         1|    A|          Python|Programming|\n",
            "|      101|         2|    B|          Python|Programming|\n",
            "|      102|         3|    A|    Data Science|  Analytics|\n",
            "|      103|         4|    C|       Databases| Technology|\n",
            "|      104|         7|    A|Business Studies| Management|\n",
            "|      102|         5|    B|    Data Science|  Analytics|\n",
            "+---------+----------+-----+----------------+-----------+\n",
            "\n",
            "+----------+----------+---+----+\n",
            "|student_id|      name|age|city|\n",
            "+----------+----------+---+----+\n",
            "|         6|Divya Nair| 20|NULL|\n",
            "+----------+----------+---+----+\n",
            "\n",
            "+---------+-----------+--------+\n",
            "|course_id|course_name|category|\n",
            "+---------+-----------+--------+\n",
            "+---------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SQL Queries / Task - 1**"
      ],
      "metadata": {
        "id": "7-Hmmj0JKvun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register DataFrames as temporary views\n",
        "students_df.createOrReplaceTempView(\"students\")\n",
        "courses_df.createOrReplaceTempView(\"courses\")\n",
        "enrollment_df.createOrReplaceTempView(\"enrollments\")\n",
        "\n",
        "# 1. All students with their course names and grades\n",
        "spark.sql(\"\"\"\n",
        "SELECT s.name, c.course_name, e.grade\n",
        "FROM enrollments e\n",
        "JOIN students s ON e.student_id = s.student_id\n",
        "JOIN courses c ON e.course_id = c.course_id\n",
        "\"\"\").show()\n",
        "\n",
        "# 2. Number of students who got grade \"A\" in each course\n",
        "spark.sql(\"\"\"\n",
        "SELECT c.course_name, COUNT(*) AS grade_A_count\n",
        "FROM enrollments e\n",
        "JOIN courses c ON e.course_id = c.course_id\n",
        "WHERE e.grade = 'A'\n",
        "GROUP BY c.course_name\n",
        "\"\"\").show()\n",
        "\n",
        "# 3. Top city with the most students enrolled in courses\n",
        "spark.sql(\"\"\"\n",
        "SELECT s.city, COUNT(*) AS total_enrolled\n",
        "FROM enrollments e\n",
        "JOIN students s ON e.student_id = s.student_id\n",
        "GROUP BY s.city\n",
        "ORDER BY total_enrolled DESC\n",
        "LIMIT 1\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFma0fd7KJvz",
        "outputId": "658e6252-1d19-43ca-925d-b0ef3abbd17c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+-----+\n",
            "|        name| course_name|grade|\n",
            "+------------+------------+-----+\n",
            "| Priya Singh|      Python|    B|\n",
            "|Rahul Sharma|      Python|    A|\n",
            "| Arjun Mehta|Data Science|    B|\n",
            "|  Aman Kumar|Data Science|    A|\n",
            "| Sneha Reddy|   Databases|    C|\n",
            "+------------+------------+-----+\n",
            "\n",
            "+----------------+-------------+\n",
            "|     course_name|grade_A_count|\n",
            "+----------------+-------------+\n",
            "|Business Studies|            1|\n",
            "|          Python|            1|\n",
            "|    Data Science|            1|\n",
            "+----------------+-------------+\n",
            "\n",
            "+---------+--------------+\n",
            "|     city|total_enrolled|\n",
            "+---------+--------------+\n",
            "|Bangalore|             1|\n",
            "+---------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"RDD-Example\").getOrCreate()\n",
        "\n",
        "# Get sparkcontext\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lldtX49LKPNx",
        "outputId": "b0e84d15-7ec8-4ad6-c806-3f33459cec5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“ **What is RDD?**\n",
        "\n",
        "**RDD** stands for **Resilient Distributed Dataset**.\n",
        "\n",
        "It is the **core data structure of Apache Spark** â€” an **immutable distributed collection of objects** that can be processed in parallel across a cluster.\n",
        "\n",
        "ðŸ‘‰ In simple words:\n",
        "\n",
        "* Think of RDD like a **giant list**, but instead of sitting in one machine, itâ€™s **spread across multiple machines**.\n",
        "* Spark can apply functions to this distributed data in parallel, making it fast and fault-tolerant.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“ **Key Properties of RDD**\n",
        "\n",
        "* **Resilient** â†’ Fault-tolerant (can recover lost data automatically using lineage).\n",
        "* **Distributed** â†’ Data is split into partitions across cluster nodes.\n",
        "* **Dataset** â†’ Collection of elements (numbers, strings, objects, rows, etc.).\n",
        "* **Immutable** â†’ Once created, cannot be changed â€” only transformed into new RDDs.\n",
        ""
      ],
      "metadata": {
        "id": "I2G89d9k-_h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from a python list\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "print(\"RDD elements:\", rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7srKze--3MV",
        "outputId": "770d6c6f-b2f0-497d-b468-e9956d77fd75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD elements: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map: Square each number\n",
        "squared_rdd = rdd.map(lambda x: x * x)\n",
        "\n",
        "#Filter: Keep only even numbers\n",
        "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n"
      ],
      "metadata": {
        "id": "HcmxlRN1_gH9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Squared:\", squared_rdd.collect())\n",
        "print(\"Even:\", even_rdd.collect())\n",
        "\n",
        "print(\"Count:\", rdd.count())\n",
        "print(\"Sum:\", rdd.sum())\n",
        "print(\"Max:\", rdd.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Xy2rZnGy_O",
        "outputId": "671e847d-2f53-4879-cf7e-25a94f3fa2e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared: [1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
            "Even: [2, 4, 6, 8]\n",
            "Count: 9\n",
            "Sum: 45\n",
            "Max: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text =[\"hello world\", \"hello spark\", \"big data with spark\"]\n",
        "\n",
        "# Create RDD\n",
        "text_rdd = sc.parallelize(text)\n",
        "\n",
        "# Split words\n",
        "words = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Map each word to (word, 1)\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "\n",
        "# Reduce by key (sum counts)\n",
        "# hello, 1 --- hello, 1\n",
        "word_count = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"Word Count:\", word_count.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx-vX50aHJh6",
        "outputId": "07f56cd6-d17a-44bc-dcfa-ef1062ecb342"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count: [('hello', 2), ('world', 1), ('big', 1), ('with', 1), ('spark', 2), ('data', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **RDD-Exercises 1**"
      ],
      "metadata": {
        "id": "7L4DbVDvasjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Setup\n",
        "spark = SparkSession.builder.appName(\"RDD-Exercises\").getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "7o01cTbfPzlT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Numbers Practice/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "6besU4t_fJ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD with numbers 1â€“15\n",
        "nums = sc.parallelize(range(1, 16))\n",
        "\n",
        "# Numbers divisible by 3\n",
        "div_by_3 = nums.filter(lambda x: x % 3 == 0)\n",
        "print(\"Divisible by 3:\", div_by_3.collect())\n",
        "\n",
        "# Each number doubled\n",
        "doubled = nums.map(lambda x: x * 2)\n",
        "print(\"Doubled:\", doubled.collect())\n",
        "\n",
        "# Count numbers greater than 10\n",
        "count_gt_10 = nums.filter(lambda x: x > 10).count()\n",
        "print(\"Count > 10:\", count_gt_10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVL4_Z7Aapma",
        "outputId": "ffc0154e-07ba-4de9-a3ea-77b1f814fe0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Divisible by 3: [3, 6, 9, 12, 15]\n",
            "Doubled: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]\n",
            "Count > 10: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **String Processing/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "_VmZtnnSfD4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fruits = sc.parallelize([\"apple\", \"banana\", \"grape\", \"banana\", \"apple\", \"mango\"])\n",
        "\n",
        "# Distinct fruits\n",
        "print(\"Distinct:\", fruits.distinct().collect())\n",
        "\n",
        "# Count each fruit\n",
        "fruit_counts = fruits.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(\"Counts:\", fruit_counts.collect())\n",
        "\n",
        "# Longest word\n",
        "longest = fruits.reduce(lambda a, b: a if len(a) > len(b) else b)\n",
        "print(\"Longest word:\", longest)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_CDQl0Va2Wz",
        "outputId": "e632c80e-dbcf-47cd-bf9f-ad8d7cb096ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct: ['apple', 'banana', 'grape', 'mango']\n",
            "Counts: [('apple', 2), ('banana', 2), ('grape', 1), ('mango', 1)]\n",
            "Longest word: banana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentence Split/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "1QU5EFw9e7uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sc.parallelize([\n",
        "    \"spark makes big data easy\",\n",
        "    \"rdd is the core of spark\",\n",
        "    \"python with spark\"\n",
        "])\n",
        "\n",
        "# Split into words\n",
        "words = sentences.flatMap(lambda s: s.split(\" \"))\n",
        "\n",
        "# Lowercase + remove duplicates\n",
        "unique_words = words.map(lambda w: w.lower()).distinct()\n",
        "print(\"Unique words:\", unique_words.collect())\n",
        "\n",
        "# Count unique words\n",
        "print(\"Total unique words:\", unique_words.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGZ806EHa7Pm",
        "outputId": "15d471a5-8a78-43a9-b53b-86ce38ef6238"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words: ['big', 'easy', 'rdd', 'core', 'of', 'python', 'with', 'spark', 'makes', 'data', 'is', 'the']\n",
            "Total unique words: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pair RDD Operations/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "rIIyq0evev46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "marks = sc.parallelize([\n",
        "    (\"Rahul\", 85), (\"Priya\", 92), (\"Aman\", 78),\n",
        "    (\"Rahul\", 90), (\"Priya\", 88)\n",
        "])\n",
        "\n",
        "# Total marks per student\n",
        "total_marks = marks.reduceByKey(lambda a, b: a + b)\n",
        "print(\"Total Marks:\", total_marks.collect())\n",
        "\n",
        "# Average marks per student\n",
        "count_marks = marks.mapValues(lambda x: (x, 1)) \\\n",
        "                   .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
        "                   .mapValues(lambda x: x[0] / x[1])\n",
        "print(\"Average Marks:\", count_marks.collect())\n",
        "\n",
        "# Student with highest marks (overall)\n",
        "highest = marks.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
        "print(\"Highest Marks:\", highest)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHFwQ7xEa-oP",
        "outputId": "ffd5e39c-c1b9-4586-b6a3-f0122768a9a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Marks: [('Rahul', 175), ('Priya', 180), ('Aman', 78)]\n",
            "Average Marks: [('Rahul', 87.5), ('Priya', 90.0), ('Aman', 78.0)]\n",
            "Highest Marks: ('Priya', 92)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reduce & Aggregate/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "BMkA-Sb3emLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nums = sc.parallelize([5, 10, 15, 20, 25])\n",
        "\n",
        "# Sum\n",
        "total_sum = nums.reduce(lambda a, b: a + b)\n",
        "print(\"Sum:\", total_sum)\n",
        "\n",
        "# Product\n",
        "product = nums.reduce(lambda a, b: a * b)\n",
        "print(\"Product:\", product)\n",
        "\n",
        "# Average (sum Ã· count)\n",
        "avg = nums.reduce(lambda a, b: a + b) / nums.count()\n",
        "print(\"Average:\", avg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnHf8gLxbCEl",
        "outputId": "249e3905-fbb7-4531-a936-772e6ca3c40c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 75\n",
            "Product: 375000\n",
            "Average: 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word Length Analysis/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "AJE3PV_6eaL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize([\"data\", \"engineering\", \"spark\", \"rdd\", \"pyspark\", \"analytics\"])\n",
        "\n",
        "# (word, length)\n",
        "word_len = words.map(lambda w: (w, len(w)))\n",
        "print(\"Word lengths:\", word_len.collect())\n",
        "\n",
        "# Longest word\n",
        "longest = words.reduce(lambda a, b: a if len(a) > len(b) else b)\n",
        "print(\"Longest word:\", longest)\n",
        "\n",
        "# Average length\n",
        "avg_len = words.map(lambda w: len(w)).reduce(lambda a, b: a + b) / words.count()\n",
        "print(\"Average length:\", avg_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzelPSjebFF_",
        "outputId": "91f8e314-e70b-415f-b480-2b9f177bdf9d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word lengths: [('data', 4), ('engineering', 11), ('spark', 5), ('rdd', 3), ('pyspark', 7), ('analytics', 9)]\n",
            "Longest word: engineering\n",
            "Average length: 6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Joins/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "3yjSPpBKeV5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = sc.parallelize([(1, \"Rahul\"), (2, \"Priya\"), (3, \"Aman\")])\n",
        "courses = sc.parallelize([(1, \"Python\"), (2, \"Spark\"), (4, \"Databases\")])\n",
        "\n",
        "# Inner Join\n",
        "inner = students.join(courses)\n",
        "print(\"Inner Join:\", inner.collect())\n",
        "\n",
        "# Left Outer Join\n",
        "left_outer = students.leftOuterJoin(courses)\n",
        "print(\"Left Outer Join:\", left_outer.collect())\n",
        "\n",
        "# Right Outer Join\n",
        "right_outer = students.rightOuterJoin(courses)\n",
        "print(\"Right Outer Join:\", right_outer.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McNYBnc2bHxO",
        "outputId": "df80107a-c8a2-4f98-f24c-54ecec5776e1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inner Join: [(1, ('Rahul', 'Python')), (2, ('Priya', 'Spark'))]\n",
            "Left Outer Join: [(1, ('Rahul', 'Python')), (2, ('Priya', 'Spark')), (3, ('Aman', None))]\n",
            "Right Outer Join: [(4, (None, 'Databases')), (1, ('Rahul', 'Python')), (2, ('Priya', 'Spark'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mini Real-World/ RDD-Exercise**"
      ],
      "metadata": {
        "id": "atmBZchobYdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = sc.parallelize([\n",
        "    (1, 200), (2, 500), (3, 300),\n",
        "    (1, 150), (2, 250)\n",
        "])\n",
        "\n",
        "# Total spend per customer\n",
        "total_spend = orders.reduceByKey(lambda a, b: a + b)\n",
        "print(\"Total Spend:\", total_spend.collect())\n",
        "\n",
        "# Customer with max spend\n",
        "max_customer = total_spend.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
        "print(\"Max Spend Customer:\", max_customer)\n",
        "\n",
        "# Total revenue\n",
        "total_revenue = orders.map(lambda x: x[1]).sum()\n",
        "print(\"Total Revenue:\", total_revenue)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbPDuqGQbMOp",
        "outputId": "bddd6375-5355-4de3-d676-a1790245795a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Spend: [(2, 750), (1, 350), (3, 300)]\n",
            "Max Spend Customer: (2, 750)\n",
            "Total Revenue: 1400\n"
          ]
        }
      ]
    }
  ]
}