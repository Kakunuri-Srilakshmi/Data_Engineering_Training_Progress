{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07663d1f-ac07-46ba-874e-fca388f025e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DeltaExample').getOrCreate()\n",
    "\n",
    "data = [(1, \"Rahul\", 50000),\n",
    "        (2, \"Priya\", 60000),\n",
    "        (3, \"Aman\", 55000)]\n",
    "cols = [\"emp_id\", \"name\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c61cef-0280-4fa6-9a9b-ab0fc6072769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Rahul</td><td>50000</td></tr><tr><td>2</td><td>Priya</td><td>60000</td></tr><tr><td>3</td><td>Aman</td><td>55000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rahul",
         50000
        ],
        [
         2,
         "Priya",
         60000
        ],
        [
         3,
         "Aman",
         55000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = spark.read.format(\"delta\").load(\"/FileStore/delta/employee\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724395d3-1769-48c0-ad3a-8badd9e3aaa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|         userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+---------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      1|2025-10-08 07:19:12|141077767851255|azuser4798_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{1650090761100692}|1008-065153-zier6...|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2025-10-08 06:52:05|141077767851255|azuser4798_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{1650090761100692}|1008-065153-zier6...|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+---------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+------+-----+------+\n|emp_id| name|salary|\n+------+-----+------+\n|     1|Rahul| 50000|\n|     2|Priya| 60000|\n|     3| Aman| 55000|\n+------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    " \n",
    "delta_emp = DeltaTable.forPath(spark, \"/FileStore/delta/employee\")\n",
    " \n",
    "updates = [(1, \"Rahul\", 52000),\n",
    "           (4, \"Sneha\", 58000)]\n",
    "update_df = spark.createDataFrame(updates, cols)\n",
    " \n",
    "delta_emp.alias(\"t\").merge(\n",
    "    update_df.alias(\"u\"),\n",
    "    \"t.emp_id = u.emp_id\"\n",
    ").whenMatchedUpdate(set={\"salary\": \"u.salary\"}) \\\n",
    ".whenNotMatchedInsert(values={\"emp_id\": \"u.emp_id\", \"name\": \"u.name\", \"salary\": \"u.salary\"}) \\\n",
    ".execute()\n",
    " \n",
    " \n",
    "delta_emp.history().show()                # Shows all versions\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/FileStore/delta/employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3cc732b-487b-41c1-98bb-aae419e382d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"OPTIMIZE delta.`/FileStore/delta/employee`\")\n",
    "spark.sql(\"VACUUM delta.`/FileStore/delta/employee` RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bb4712-3734-4759-975b-03afec0bbe57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------+-----+---------+\n|order_id|   product|region|quantity|price|   status|\n+--------+----------+------+--------+-----+---------+\n|       1|    Laptop| North|       2|55000|Completed|\n|       2|    Mobile| South|       3|25000|Completed|\n|       3|      Book| North|      10|  700|  Pending|\n|       4|Headphones|  East|       5| 3000|Completed|\n+--------+----------+------+--------+-----+---------+\n\n+--------+----------+------+--------+-----+---------+------------+\n|order_id|   product|region|quantity|price|   status|total_amount|\n+--------+----------+------+--------+-----+---------+------------+\n|       1|    Laptop| North|       2|55000|Completed|      110000|\n|       2|    Mobile| South|       3|25000|Completed|       75000|\n|       4|Headphones|  East|       5| 3000|Completed|       15000|\n+--------+----------+------+--------+-----+---------+------------+\n\n+------+-----------+\n|region|total_sales|\n+------+-----------+\n| North|     110000|\n| South|      75000|\n|  East|      15000|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# \uD83D\uDFE4 Bronze - Raw Data\n",
    "data = [\n",
    "    Row(order_id=1, product=\"Laptop\", region=\"North\", quantity=2, price=55000, status=\"Completed\"),\n",
    "    Row(order_id=2, product=\"Mobile\", region=\"South\", quantity=3, price=25000, status=\"Completed\"),\n",
    "    Row(order_id=3, product=\"Book\", region=\"North\", quantity=10, price=700, status=\"Pending\"),\n",
    "    Row(order_id=4, product=\"Headphones\", region=\"East\", quantity=5, price=3000, status=\"Completed\")\n",
    "]\n",
    "bronze_df = spark.createDataFrame(data)\n",
    "bronze_df.show()\n",
    "\n",
    "# \uD83D\uDFE2 Silver - Filter + Derived Column\n",
    "silver_df = bronze_df.filter(col(\"status\") == \"Completed\") \\\n",
    "                     .withColumn(\"total_amount\", col(\"quantity\") * col(\"price\"))\n",
    "silver_df.show()\n",
    "\n",
    "# \uD83D\uDFE1 Gold - Aggregate\n",
    "gold_df = silver_df.groupBy(\"region\") \\\n",
    "                   .agg(sum(\"total_amount\").alias(\"total_sales\"))\n",
    "gold_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure_Databricks_Delta Lake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}