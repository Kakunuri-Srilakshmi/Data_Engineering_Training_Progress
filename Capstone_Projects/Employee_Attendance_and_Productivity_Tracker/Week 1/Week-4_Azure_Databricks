
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, avg, count, sum as _sum
from pyspark.sql.types import DoubleType, TimestampType

# Start Spark session
# spark = SparkSession.builder.appName("Week4_ETL").getOrCreate()

# Load CSV files
attendance_df = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/tables/attendance.csv")
tasks_df = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/tables/tasks.csv")
employees_df = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/tables/employees.csv")

# Convert timestamps
attendance_df = attendance_df.withColumn("clock_in", col("clock_in").cast(TimestampType()))
attendance_df = attendance_df.withColumn("clock_out", col("clock_out").cast(TimestampType()))

# Compute work hours, break time, net hours
attendance_df = attendance_df.withColumn(
    "workhours",
    (col("clock_out").cast("double") - col("clock_in").cast("double")) / 3600
)
attendance_df = attendance_df.withColumn("breaktime", when(col("workhours") > 8, 1.0).otherwise(0.5))
attendance_df = attendance_df.withColumn("net_workhours", (col("workhours") - col("breaktime")).cast(DoubleType()))
attendance_df = attendance_df.withColumn("net_workhours", when(col("net_workhours") < 0, 0).otherwise(col("net_workhours")))

# Compute tasks completed per employee
tasks_completed_df = tasks_df.filter(col("status").rlike("(?i)completed")) \
    .groupBy("employee_id").count().withColumnRenamed("count", "tasks_completed")

# Merge attendance with tasks
df = attendance_df.join(tasks_completed_df, on="employee_id", how="left")
df = df.withColumn("tasks_completed", when(col("tasks_completed").isNull(), 0).otherwise(col("tasks_completed")))
df = df.withColumn("productivity_score", col("tasks_completed") / col("net_workhours"))
df = df.withColumn("productivity_score", when(col("productivity_score").isNull(), 0).otherwise(col("productivity_score")))

# Merge employee info (department)
df = df.join(employees_df.select("employee_id", "department"), on="employee_id", how="left")

# Compute department-level KPIs
department_kpi = df.groupBy("department").agg(
    avg("net_workhours").alias("avg_net_workhours"),
    avg("productivity_score").alias("avg_productivity"),
    count("employee_id").alias("total_employees"),
    _sum("tasks_completed").alias("total_tasks_completed")
)

# Show results
print("=== Department-Level KPIs ===")
department_kpi.show(truncate=False)

# Save output as Delta or CSV for dashboard use
# Delta format
department_kpi.write.format("delta").mode("overwrite").save("/FileStore/tables/department_kpi_delta")

# CSV format 
department_kpi.write.option("header", True).mode("overwrite").csv("/FileStore/tables/department_kpi_csv")

print(" Department KPIs saved successfully!")
