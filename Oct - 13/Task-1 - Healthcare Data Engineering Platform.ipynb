{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b8159ef-46f4-49fc-baee-b695b08aaeea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 143 bytes.\nWrote 266 bytes.\nWrote 275 bytes.\nWrote 236 bytes.\n+-------------+-------------+\n|hospital_name|total_revenue|\n+-------------+-------------+\n|    City Care|          400|\n|     CureWell|          300|\n|     LifePlus|          250|\n+-------------+-------------+\n\n+---------------+--------------+\n|hospital_region|total_patients|\n+---------------+--------------+\n|           West|             1|\n|          North|             1|\n|          South|             1|\n+---------------+--------------+\n\n+---------+----------+\n|diagnosis|total_cost|\n+---------+----------+\n| Diabetes|       400|\n|  Allergy|       300|\n|      Flu|       250|\n+---------+----------+\n\n+-------------+---------------+-------------+----------+----------------+\n|hospital_name|hospital_region|    diagnosis|total_cost|num_appointments|\n+-------------+---------------+-------------+----------+----------------+\n|     MediHope|           East|   Cardiology|      1500|               1|\n|     MediHope|           East|Heart Disease|      1000|               1|\n|    City Care|          North|     Diabetes|       850|               2|\n|     LifePlus|          South|          Flu|       250|               1|\n|     CureWell|           West|      Allergy|       300|               1|\n+-------------+---------------+-------------+----------+----------------+\n\n+-------------+---------------+---------+----------+----------------+\n|hospital_name|hospital_region|diagnosis|total_cost|num_appointments|\n+-------------+---------------+---------+----------+----------------+\n|     LifePlus|          South|      Flu|       250|               1|\n|     CureWell|           West|  Allergy|       300|               1|\n|    City Care|          North| Diabetes|       400|               1|\n+-------------+---------------+---------+----------+----------------+\n\n+-------------+-------+\n|hospital_name|revenue|\n+-------------+-------+\n|    City Care|    850|\n|     MediHope|   2500|\n|     CureWell|    300|\n|     LifePlus|    250|\n+-------------+-------+\n\n+-------------+--------+\n|    diagnosis|avg_cost|\n+-------------+--------+\n|Heart Disease|  1000.0|\n|     Diabetes|   425.0|\n|   Cardiology|  1500.0|\n|          Flu|   250.0|\n|      Allergy|   300.0|\n+-------------+--------+\n\n+---------------+------------+\n|hospital_region|num_patients|\n+---------------+------------+\n|           East|           2|\n|          North|           2|\n|           West|           1|\n|          South|           1|\n+---------------+------------+\n\n+----+-----+----------------+\n|year|month|num_appointments|\n+----+-----+----------------+\n|2024|    1|               4|\n|2024|    2|               2|\n+----+-----+----------------+\n\n+---------+----------+\n|diagnosis|total_cost|\n+---------+----------+\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sum as _sum, avg, count, desc, current_date, date_sub\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"MediPulseCapstone\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "\n",
    "# Create Sample Files (DBFS)\n",
    "\n",
    "base_path = \"/tmp/medi_pulse/\"\n",
    "dbutils.fs.rm(base_path, True)\n",
    "dbutils.fs.mkdirs(base_path)\n",
    "\n",
    "# Patients CSV\n",
    "patients_csv = \"\"\"patient_id,name,age,gender,region\n",
    "P001,Arjun Mehta,34,M,North\n",
    "P002,Neha Sharma,29,F,South\n",
    "P003,Rahul Gupta,40,M,East\n",
    "P004,Sneha Nair,25,F,West\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"patients.csv\", patients_csv, overwrite=True)\n",
    "\n",
    "# Hospitals JSON\n",
    "hospitals_json = '\\n'.join([\n",
    "    '{\"hospital_id\":\"H001\",\"hospital_name\":\"City Care\",\"region\":\"North\"}',\n",
    "    '{\"hospital_id\":\"H002\",\"hospital_name\":\"LifePlus\",\"region\":\"South\"}',\n",
    "    '{\"hospital_id\":\"H003\",\"hospital_name\":\"MediHope\",\"region\":\"East\"}',\n",
    "    '{\"hospital_id\":\"H004\",\"hospital_name\":\"CureWell\",\"region\":\"West\"}'\n",
    "])\n",
    "dbutils.fs.put(base_path + \"hospitals.json\", hospitals_json, overwrite=True)\n",
    "\n",
    "# Appointments Day 1 CSV\n",
    "appointments_day1 = \"\"\"appointment_id,patient_id,hospital_id,appointment_date,diagnosis,cost,status\n",
    "A1001,P001,H001,2024-01-10,Diabetes,400,Completed\n",
    "A1002,P002,H002,2024-01-11,Flu,250,Completed\n",
    "A1003,P003,H003,2024-01-11,Heart Disease,1000,Pending\n",
    "A1004,P004,H004,2024-01-12,Allergy,300,Completed\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"appointments_day1.csv\", appointments_day1, overwrite=True)\n",
    "\n",
    "# Appointments Day 2 CSV\n",
    "appointments_day2 = \"\"\"appointment_id,patient_id,hospital_id,appointment_date,diagnosis,cost,status\n",
    "A1005,P001,H001,2024-02-05,Diabetes,450,Completed\n",
    "A1006,P003,H003,2024-02-06,Cardiology,1500,Completed\n",
    "A1003,P003,H003,2024-01-11,Heart Disease,1000,Completed\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"appointments_day2.csv\", appointments_day2, overwrite=True)\n",
    "\n",
    "# File paths\n",
    "raw_patients = base_path + \"patients.csv\"\n",
    "raw_hospitals = base_path + \"hospitals.json\"\n",
    "raw_appointments = base_path + \"appointments_day1.csv\"\n",
    "raw_appointments_day2 = base_path + \"appointments_day2.csv\"\n",
    "\n",
    "\n",
    "# Bronze Layer: Raw Ingestion\n",
    "bronze_patients = spark.read.csv(raw_patients, header=True, inferSchema=True)\n",
    "bronze_hospitals = spark.read.json(raw_hospitals)\n",
    "bronze_appointments = spark.read.csv(raw_appointments, header=True, inferSchema=True)\n",
    "\n",
    "bronze_patients.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/bronze_patients\")\n",
    "bronze_hospitals.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/bronze_hospitals\")\n",
    "bronze_appointments.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/bronze_appointments\")\n",
    "\n",
    "\n",
    "# Silver Layer: Data Cleansing & Transformation\n",
    "\n",
    "# Rename columns to avoid duplicates\n",
    "patients_df = bronze_patients.withColumnRenamed(\"region\", \"patient_region\")\n",
    "hospitals_df = bronze_hospitals.withColumnRenamed(\"region\", \"hospital_region\")\n",
    "\n",
    "# Alias tables to avoid ambiguity after join\n",
    "patients_alias = patients_df.alias(\"p\")\n",
    "hospitals_alias = hospitals_df.alias(\"h\")\n",
    "\n",
    "silver_appointments = bronze_appointments.filter(col(\"status\") == \"Completed\") \\\n",
    "    .join(patients_alias, \"patient_id\") \\\n",
    "    .join(hospitals_alias, \"hospital_id\") \\\n",
    "    .withColumn(\"year\", year(col(\"appointment_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"appointment_date\"))) \\\n",
    "    .select(\n",
    "        col(\"appointment_id\"),\n",
    "        col(\"patient_id\"),\n",
    "        col(\"hospital_id\"),\n",
    "        col(\"appointment_date\"),\n",
    "        col(\"diagnosis\"),\n",
    "        col(\"cost\"),\n",
    "        col(\"status\"),\n",
    "        col(\"p.patient_region\").alias(\"patient_region\"),\n",
    "        col(\"h.hospital_region\").alias(\"hospital_region\"),\n",
    "        col(\"h.hospital_name\").alias(\"hospital_name\"),\n",
    "        col(\"year\"),\n",
    "        col(\"month\")\n",
    "    )\n",
    "\n",
    "silver_appointments.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_appointments\")\n",
    "\n",
    "# Gold Layer: Analytical Aggregations\n",
    "\n",
    "gold_revenue = silver_appointments.groupBy(\"hospital_name\").agg(_sum(\"cost\").alias(\"total_revenue\"))\n",
    "gold_revenue.show()\n",
    "\n",
    "gold_patients_region = silver_appointments.groupBy(\"hospital_region\").agg(count(\"patient_id\").alias(\"total_patients\"))\n",
    "gold_patients_region.show()\n",
    "\n",
    "gold_top_diagnosis = silver_appointments.groupBy(\"diagnosis\") \\\n",
    "    .agg(_sum(\"cost\").alias(\"total_cost\")) \\\n",
    "    .orderBy(desc(\"total_cost\")) \\\n",
    "    .limit(3)\n",
    "gold_top_diagnosis.show()\n",
    "\n",
    "gold_summary = silver_appointments.groupBy(\"hospital_name\", \"hospital_region\", \"diagnosis\") \\\n",
    "    .agg(_sum(\"cost\").alias(\"total_cost\"), count(\"appointment_id\").alias(\"num_appointments\"))\n",
    "gold_summary.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/gold_healthcare_summary\")\n",
    "\n",
    "\n",
    "# Incremental Load Simulation\n",
    "\n",
    "appointments_day2_df = spark.read.csv(raw_appointments_day2, header=True, inferSchema=True)\n",
    "\n",
    "silver_table = DeltaTable.forPath(spark, \"/tmp/silver_appointments\")\n",
    "silver_table.alias(\"silver\").merge(\n",
    "    appointments_day2_df.alias(\"new\"),\n",
    "    \"silver.appointment_id = new.appointment_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"patient_id\": col(\"new.patient_id\"),\n",
    "        \"hospital_id\": col(\"new.hospital_id\"),\n",
    "        \"appointment_date\": col(\"new.appointment_date\"),\n",
    "        \"diagnosis\": col(\"new.diagnosis\"),\n",
    "        \"cost\": col(\"new.cost\"),\n",
    "        \"status\": col(\"new.status\")\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"appointment_id\": col(\"new.appointment_id\"),\n",
    "        \"patient_id\": col(\"new.patient_id\"),\n",
    "        \"hospital_id\": col(\"new.hospital_id\"),\n",
    "        \"appointment_date\": col(\"new.appointment_date\"),\n",
    "        \"diagnosis\": col(\"new.diagnosis\"),\n",
    "        \"cost\": col(\"new.cost\"),\n",
    "        \"status\": col(\"new.status\")\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "updated_silver = spark.read.format(\"delta\").load(\"/tmp/silver_appointments\") \\\n",
    "    .join(patients_alias, \"patient_id\") \\\n",
    "    .join(hospitals_alias, \"hospital_id\") \\\n",
    "    .withColumn(\"year\", year(col(\"appointment_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"appointment_date\"))) \\\n",
    "    .select(\n",
    "        col(\"appointment_id\"),\n",
    "        col(\"patient_id\"),\n",
    "        col(\"hospital_id\"),\n",
    "        col(\"appointment_date\"),\n",
    "        col(\"diagnosis\"),\n",
    "        col(\"cost\"),\n",
    "        col(\"status\"),\n",
    "        col(\"p.patient_region\").alias(\"patient_region\"),\n",
    "        col(\"h.hospital_region\").alias(\"hospital_region\"),\n",
    "        col(\"h.hospital_name\").alias(\"hospital_name\"),\n",
    "        col(\"year\"),\n",
    "        col(\"month\")\n",
    "    )\n",
    "\n",
    "updated_gold_summary = updated_silver.groupBy(\"hospital_name\", \"hospital_region\", \"diagnosis\") \\\n",
    "    .agg(_sum(\"cost\").alias(\"total_cost\"), count(\"appointment_id\").alias(\"num_appointments\"))\n",
    "updated_gold_summary.show()\n",
    "\n",
    "\n",
    "# Delta Lake Features\n",
    "\n",
    "gold_before_update = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/gold_healthcare_summary\")\n",
    "gold_before_update.show()\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/gold_healthcare_summary\")\n",
    "delta_table.vacuum(168)\n",
    "delta_table.optimize().executeZOrderBy(\"hospital_name\")\n",
    "\n",
    "# Analytical Queries\n",
    "\n",
    "# Total revenue per hospital\n",
    "updated_gold_summary.groupBy(\"hospital_name\").agg(_sum(\"total_cost\").alias(\"revenue\")).show()\n",
    "\n",
    "# Average cost per diagnosis\n",
    "updated_silver.groupBy(\"diagnosis\").agg(avg(\"cost\").alias(\"avg_cost\")).show()\n",
    "\n",
    "# Number of patients per hospital region\n",
    "updated_silver.groupBy(\"hospital_region\").agg(count(\"patient_id\").alias(\"num_patients\")).show()\n",
    "\n",
    "# Trend of appointments month-over-month\n",
    "updated_silver.groupBy(\"year\",\"month\").agg(count(\"appointment_id\").alias(\"num_appointments\")) \\\n",
    "    .orderBy(\"year\",\"month\").show()\n",
    "\n",
    "# Top 5 most expensive treatments last 6 months\n",
    "six_months_ago = date_sub(current_date(), 180)\n",
    "updated_silver.filter(col(\"appointment_date\") >= six_months_ago) \\\n",
    "    .groupBy(\"diagnosis\") \\\n",
    "    .agg(_sum(\"cost\").alias(\"total_cost\")) \\\n",
    "    .orderBy(desc(\"total_cost\")) \\\n",
    "    .limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f0f5116-3128-4543-8d50-78207fa8484c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task-1 - Healthcare Data Engineering Platform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}