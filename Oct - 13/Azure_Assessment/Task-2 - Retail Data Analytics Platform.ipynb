{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23bd93d-7729-4b41-baf8-5107e3e80ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 176 bytes.\nWrote 232 bytes.\nWrote 184 bytes.\nWrote 363 bytes.\nSilver Orders — Version 0\n+--------+-----------+-------------+---------------+----------+-----------+--------+-------+------------+---------+----------+-----------------+\n|order_id|customer_id|customer_name|customer_region|   product|   category|quantity|  price|total_amount|   status|order_date|            email|\n+--------+-----------+-------------+---------------+----------+-----------+--------+-------+------------+---------+----------+-----------------+\n|    1001|          1|    Arjun Rao|          North|    Laptop|Electronics|       2|55000.0|    110000.0|Completed|2024-01-15|arjun@example.com|\n|    1002|          2|  Sneha Patel|          South|    Mobile|Electronics|       3|25000.0|     75000.0|Completed|2024-01-16|sneha@example.com|\n|    1004|          1|    Arjun Rao|          North|Headphones|Accessories|       5| 3000.0|     15000.0|Completed|2024-01-17|arjun@example.com|\n+--------+-----------+-------------+---------------+----------+-----------+--------+-------+------------+---------+----------+-----------------+\n\nSilver Orders — After Optimization & Z-Ordering\n+--------+-----------+-------------+---------------+----------+-----------+--------+-------+------------+---------+----------+-----------------+\n|order_id|customer_id|customer_name|customer_region|   product|   category|quantity|  price|total_amount|   status|order_date|            email|\n+--------+-----------+-------------+---------------+----------+-----------+--------+-------+------------+---------+----------+-----------------+\n|    1001|          1|    Arjun Rao|          North|    Laptop|Electronics|       2|55000.0|    110000.0|Completed|2024-01-15|arjun@example.com|\n|    1002|          2|  Sneha Patel|          South|    Mobile|Electronics|       3|25000.0|     75000.0|Completed|2024-01-16|sneha@example.com|\n|    1004|          1|    Arjun Rao|          North|Headphones|Accessories|       5| 3000.0|     15000.0|Completed|2024-01-17|arjun@example.com|\n|    1005|          4|    Neha Iyer|           West|    Mobile|Electronics|       1|25000.0|     25000.0|Completed|2024-01-18| neha@example.com|\n|    1006|          2|  Sneha Patel|          South|      Book| Stationery|       2|  700.0|      1400.0|Completed|2024-01-19|sneha@example.com|\n|    1003|          3| Rahul Sharma|           East|      Book| Stationery|      10|  700.0|      7000.0|Completed|2024-01-16|rahul@example.com|\n+--------+-----------+-------------+---------------+----------+-----------+--------+-------+------------+---------+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, desc, row_number, sum as _sum, avg, count, year, month, current_date, date_sub\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Retail360Capstone\").getOrCreate()\n",
    "\n",
    "# Base path\n",
    "base_path = \"/tmp/retail360/\"\n",
    "dbutils.fs.rm(base_path, True)\n",
    "dbutils.fs.mkdirs(base_path)\n",
    "\n",
    "# Create sample files\n",
    "\n",
    "# customers.csv\n",
    "customers_csv = \"\"\"customer_id,name,region,email\n",
    "1,Arjun Rao,North,arjun@example.com\n",
    "2,Sneha Patel,South,sneha@example.com\n",
    "3,Rahul Sharma,East,rahul@example.com\n",
    "4,Neha Iyer,West,neha@example.com\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"customers.csv\", customers_csv, overwrite=True)\n",
    "\n",
    "# orders_day1.csv\n",
    "orders_day1 = \"\"\"order_id,customer_id,product,quantity,price,status,order_date\n",
    "1001,1,Laptop,2,55000,Completed,2024-01-15\n",
    "1002,2,Mobile,3,25000,Completed,2024-01-16\n",
    "1003,3,Book,10,700,Pending,2024-01-16\n",
    "1004,1,Headphones,5,3000,Completed,2024-01-17\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"orders_day1.csv\", orders_day1, overwrite=True)\n",
    "\n",
    "# orders_day2.csv\n",
    "orders_day2 = \"\"\"order_id,customer_id,product,quantity,price,status,order_date\n",
    "1005,4,Mobile,1,25000,Completed,2024-01-18\n",
    "1006,2,Book,2,700,Completed,2024-01-19\n",
    "1003,3,Book,10,700,Completed,2024-01-16\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"orders_day2.csv\", orders_day2, overwrite=True)\n",
    "\n",
    "# products.json\n",
    "products_json = \"\"\"{\"product_id\":\"P001\",\"product_name\":\"Laptop\",\"category\":\"Electronics\",\"product\":\"Laptop\"}\n",
    "{\"product_id\":\"P002\",\"product_name\":\"Mobile\",\"category\":\"Electronics\",\"product\":\"Mobile\"}\n",
    "{\"product_id\":\"P003\",\"product_name\":\"Book\",\"category\":\"Stationery\",\"product\":\"Book\"}\n",
    "{\"product_id\":\"P004\",\"product_name\":\"Headphones\",\"category\":\"Accessories\",\"product\":\"Headphones\"}\n",
    "\"\"\"\n",
    "dbutils.fs.put(base_path + \"products.json\", products_json, overwrite=True)\n",
    "\n",
    "# Bronze Layer\n",
    "\n",
    "customers_raw = spark.read.csv(base_path + \"customers.csv\", header=True, inferSchema=True)\n",
    "orders_raw = spark.read.csv(base_path + \"orders_day1.csv\", header=True, inferSchema=True)\n",
    "products_raw = spark.read.json(base_path + \"products.json\")\n",
    "\n",
    "customers_raw.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/bronze_customers\")\n",
    "orders_raw.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/bronze_orders\")\n",
    "products_raw.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/bronze_products\")\n",
    "\n",
    "\n",
    "# Silver Layer — Cleanse & Transform\n",
    "\n",
    "customers_df = customers_raw.withColumnRenamed(\"region\",\"customer_region\")\n",
    "orders_df = orders_raw.filter(col(\"status\") == \"Completed\") \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .withColumn(\"order_date\", col(\"order_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"total_amount\", expr(\"quantity * price\"))\n",
    "\n",
    "silver_orders = orders_df.alias(\"o\") \\\n",
    "    .join(customers_df.alias(\"c\"), \"customer_id\") \\\n",
    "    .join(products_raw.select(\"product_name\",\"category\").alias(\"p\"), col(\"o.product\")==col(\"p.product_name\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"c.name\").alias(\"customer_name\"),\n",
    "        col(\"customer_region\"),\n",
    "        col(\"o.product\"),\n",
    "        col(\"p.category\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"price\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"status\"),\n",
    "        col(\"order_date\"),\n",
    "        col(\"c.email\")\n",
    "    )\n",
    "\n",
    "silver_orders.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/silver_orders\")\n",
    "\n",
    "\n",
    "# Gold Layer — Aggregations\n",
    "\n",
    "revenue_by_region = silver_orders.groupBy(\"customer_region\").agg(_sum(\"total_amount\").alias(\"total_revenue\"))\n",
    "revenue_by_region.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/gold_revenue_by_region\")\n",
    "\n",
    "product_sales = silver_orders.groupBy(\"product\",\"category\") \\\n",
    "    .agg(_sum(\"total_amount\").alias(\"revenue\"), _sum(\"quantity\").alias(\"units_sold\"))\n",
    "w = Window.partitionBy(\"category\").orderBy(desc(\"revenue\"))\n",
    "product_sales_ranked = product_sales.withColumn(\"rank_by_revenue\", row_number().over(w))\n",
    "product_sales_ranked.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/gold_product_sales\")\n",
    "\n",
    "\n",
    "# Incremental Load — MERGE day2\n",
    "\n",
    "new_orders_df = spark.read.csv(base_path + \"orders_day2.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\")) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .withColumn(\"order_date\", col(\"order_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"total_amount\", expr(\"quantity * price\"))\n",
    "\n",
    "silver_dt = DeltaTable.forPath(spark, \"/tmp/silver_orders\")\n",
    "staging = new_orders_df.alias(\"s\").join(customers_df.alias(\"c\"), \"customer_id\") \\\n",
    "    .join(products_raw.select(\"product_name\",\"category\").alias(\"p\"), col(\"s.product\")==col(\"p.product_name\"), \"left\") \\\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"c.name\").alias(\"customer_name\"),\n",
    "        col(\"customer_region\"),\n",
    "        col(\"s.product\"),\n",
    "        col(\"p.category\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"price\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"status\"),\n",
    "        col(\"order_date\"),\n",
    "        col(\"c.email\")\n",
    "    )\n",
    "\n",
    "(silver_dt.alias(\"t\")\n",
    " .merge(staging.alias(\"s\"), \"t.order_id = s.order_id\")\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute())\n",
    "\n",
    "\n",
    "# Delta Lake Features: Time Travel, Optimize, VACUUM\n",
    "\n",
    "# Time Travel Example (version 0)\n",
    "silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\",0).load(\"/tmp/silver_orders\")\n",
    "print(\"Silver Orders — Version 0\")\n",
    "silver_v0.show()\n",
    "\n",
    "# VACUUM old versions\n",
    "silver_dt.vacuum(168)\n",
    "\n",
    "# Optimize + Z-Ordering (metrics suppressed)\n",
    "silver_dt.optimize().executeZOrderBy(\"customer_id\")\n",
    "\n",
    "# Read table after optimization to see data\n",
    "silver_orders_post_opt = spark.read.format(\"delta\").load(\"/tmp/silver_orders\")\n",
    "print(\"Silver Orders — After Optimization & Z-Ordering\")\n",
    "silver_orders_post_opt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task-2 - Retail Data Analytics Platform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}